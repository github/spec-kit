---
description: Run integration tests by starting services, executing acceptance scenarios, and reporting results
handoffs:
  - label: Diagnose & Fix
    agent: speckit.fix
    prompt: Diagnose why the feature is failing and create a correction plan
  - label: Quick Fix
    agent: speckit.implement
    prompt: Fix the validation issues found (use when root cause is clear)
  - label: Update Tasks
    agent: speckit.review
    prompt: Add correction tasks for validation failures
scripts:
  sh: scripts/bash/check-prerequisites.sh --json
  ps: scripts/powershell/check-prerequisites.ps1 -Json
---

# Integration Validation

You are a **QA Engineer**. Your job is to validate the implementation by running the acceptance scenarios defined in the specification.

## User Input

```text
$ARGUMENTS
```

Consider user input for scope (specific user story, full validation, quick smoke test).

---

## Prerequisites

This command requires:

1. **MCP Server configured** - Run `/speckit.mcp` first if not done
2. **Specification with acceptance scenarios** - `spec.md` with User Stories
3. **Implementation complete** - Tasks marked as done in `tasks.md`

---

## Validation Modes

| Mode | Trigger | Scope |
|------|---------|-------|
| **Full** | "full", "all", default | All user stories in priority order |
| **Story** | "US1", "story 2", "P1" | Specific user story only |
| **Smoke** | "smoke", "quick" | P1 story happy path only |
| **API Only** | "api", "backend" | API endpoints without browser |
| **UI Only** | "ui", "frontend" | Browser tests only |

---

## Phase 1: Preparation

### Step 1.1: Load Context

Run `{SCRIPT}` to get paths, then load:

```
FEATURE_DIR/
‚îú‚îÄ‚îÄ spec.md          ‚Üí User stories with acceptance scenarios
‚îú‚îÄ‚îÄ plan.md          ‚Üí Technical implementation details
‚îú‚îÄ‚îÄ tasks.md         ‚Üí Implementation progress
‚îú‚îÄ‚îÄ contracts/       ‚Üí API contracts for endpoint testing
‚îî‚îÄ‚îÄ quickstart.md    ‚Üí How to run the application
```

### Step 1.2: Parse Acceptance Scenarios

Extract testable scenarios from `spec.md`:

```markdown
### User Story 1 - [Title] (Priority: P1)

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]
2. **Given** [initial state], **When** [action], **Then** [expected outcome]
```

Build a test matrix:

| Story | Scenario | Type | Steps | Status |
|-------|----------|------|-------|--------|
| US1 | Login success | UI+API | 5 | Pending |
| US1 | Login invalid | UI | 3 | Pending |
| US2 | Create order | API | 4 | Pending |

### Step 1.3: Check Implementation Progress

Read `tasks.md` to determine what's testable:

```markdown
## Testable Scope

Based on completed tasks:
- ‚úÖ US1 - User Authentication (all tasks complete)
- ‚úÖ US2 - Order Management (all tasks complete)
- ‚ö†Ô∏è US3 - Reporting (3/5 tasks complete - partial testing)
- ‚ùå US4 - Admin Panel (not started - skip)
```

---

## Phase 2: Environment Setup

### Step 2.1: Start Infrastructure

Using MCP tools (or bash fallback):

```
1. start_docker          ‚Üí Start DB, Redis, etc.
2. Wait for containers   ‚Üí health_check on each
3. start_service backend ‚Üí Start backend service
4. Wait for backend      ‚Üí health_check backend
5. start_service frontend ‚Üí Start frontend
6. Wait for frontend     ‚Üí health_check frontend
```

**Important**: Wait for each service to be healthy before proceeding.

### Step 2.2: Verify Environment

Run health checks on all services:

```markdown
## Environment Status

| Service | Status | URL |
|---------|--------|-----|
| Database | ‚úÖ Running | localhost:5432 |
| Redis | ‚úÖ Running | localhost:6379 |
| Backend | ‚úÖ Healthy | http://localhost:8080/health |
| Frontend | ‚úÖ Ready | http://localhost:5173 |
```

If any service fails:
1. Check logs: `service_logs <name> 50`
2. Report the error
3. Ask user how to proceed

### Step 2.3: Seed Test Data (if needed)

If `quickstart.md` specifies seed data:

```bash
# Run migrations
npm run db:migrate

# Seed test data
npm run db:seed
```

Or via API:
```
api_post /api/test/seed
```

---

## Phase 3: Execute Validation

**CRITICAL**: Track validation execution status. The final report MUST reflect reality:
- If validation was interrupted ‚Üí Report as **INCOMPLETE**
- If scenarios could not be executed ‚Üí Report as **ERROR**
- If all scenarios ran but some failed ‚Üí Report as **FAILED** with details
- ONLY report **PASSED** if ALL scenarios were executed AND passed

### Validation Execution Tracking

Maintain a running status during validation:

```markdown
## Validation Execution Log

| Timestamp | Event | Status | Details |
|-----------|-------|--------|---------|
| {time} | Start validation | ‚è≥ Running | Mode: {mode} |
| {time} | US1 Scenario 1 | ‚úÖ Pass | 2.3s |
| {time} | US1 Scenario 2 | ‚ùå Fail | Element not found |
| {time} | US2 Scenario 1 | ‚ö†Ô∏è Error | Service unavailable |
| {time} | Validation ended | ‚ùå Incomplete | Stopped at US2 due to error |
```

### Step 3.1: For Each User Story (by priority)

Process stories in order: P1 ‚Üí P2 ‚Üí P3

```markdown
## Validating: US1 - User Authentication

### Scenario 1.1: Successful login

**Given**: A registered user exists
**When**: User enters valid credentials and clicks login
**Then**: User is redirected to dashboard

#### Steps:
```

### Step 3.2: Execute Scenario Steps

For each scenario, translate Gherkin to MCP actions:

**Given** (Setup):
```
# Create test data if needed
api_post /api/test/users {"email": "test@example.com", "password": "secret"}
```

**When** (Actions):
```
# UI Actions
browser_open /login
browser_fill #email test@example.com
browser_fill #password secret
browser_click button[type=submit]

# Or API Actions
api_post /api/auth/login {"email": "test@example.com", "password": "secret"}
```

**Then** (Assertions):
```
# UI Assertions
browser_wait_for .dashboard
browser_url ‚Üí should contain "/dashboard"
browser_exists .welcome-message ‚Üí should be true

# API Assertions
‚Üí status should be 200
‚Üí body should contain { "token": "..." }
```

### Step 3.3: Capture Evidence

For each scenario:

1. **Screenshot** on success: `browser_screenshot`
2. **Screenshot** on failure: `browser_screenshot`
3. **API Response**: Log response body
4. **Logs on failure**: `service_logs backend 20 "ERROR"`

### Step 3.4: Handle Failures

When a step fails:

```markdown
### ‚ùå Scenario 1.2: Login with invalid password - FAILED

**Failed at step**: Then user sees error message

**Expected**: Element `.error-message` exists with text "Invalid credentials"
**Actual**: Element `.error-message` not found

**Evidence**:
- Screenshot: [attached]
- Backend logs (last 10 lines with ERROR):
  ```
  [ERROR] AuthController: NullPointerException at line 45
  ```

**Probable Cause**: Backend error handling not implemented

**Suggested Fix**: Check `AuthController.java:45` for null check
```

Continue with remaining scenarios unless critical failure.

### Step 3.5: Handle Execution Errors

**IMPORTANT**: Distinguish between test failures and execution errors:

| Type | Example | Action |
|------|---------|--------|
| **Test Failure** | Assertion failed, element not found | Record failure, continue testing |
| **Execution Error** | Service crashed, timeout, MCP error | Record error, attempt recovery or stop |
| **Critical Error** | Infrastructure down, cannot proceed | Stop validation, report incomplete |

When an execution error occurs:

```markdown
### ‚ö†Ô∏è EXECUTION ERROR at US2 Scenario 1

**Error Type**: Service Unavailable
**Error Message**: Backend returned 503 after 3 retries
**Impact**: Cannot continue testing US2 and beyond

**Recovery Attempted**:
- Restart backend service: Failed
- Wait 30s and retry: Failed

**Decision**: Marking validation as INCOMPLETE
```

**NEVER mark validation as successful if execution errors occurred.** The report must clearly state that not all scenarios could be tested.

---

## Phase 4: Results & Reporting

### Step 4.1: Determine Overall Validation Status

**CRITICAL**: Before generating the report, determine the TRUE validation status:

```markdown
## Validation Status Determination

Check these conditions IN ORDER:

1. **Were there execution errors that prevented testing?**
   - YES ‚Üí Status: ‚ùå **INCOMPLETE** - Cannot determine feature quality
   - NO ‚Üí Continue to step 2

2. **Were all planned scenarios executed?**
   - NO ‚Üí Status: ‚ö†Ô∏è **PARTIAL** - Some scenarios not tested
   - YES ‚Üí Continue to step 3

3. **Did any scenarios fail?**
   - YES ‚Üí Status: ‚ùå **FAILED** - Feature has issues that must be fixed
   - NO ‚Üí Status: ‚úÖ **PASSED** - All scenarios passed
```

**IMPORTANT**:
- ONLY report "PASSED" or "All Green" if EVERY scenario was executed AND passed
- If validation was interrupted, clearly state what was NOT tested
- The fix command relies on this report being accurate

### Step 4.2: Generate Validation Report

Create `FEATURE_DIR/validation/report-{date}.md`:

```markdown
# Validation Report: [Feature Name]

**Date**: {current_date}
**Scope**: {validation_mode}
**Duration**: {total_time}
**Status**: {PASSED|FAILED|INCOMPLETE|PARTIAL}

## ‚ö†Ô∏è Validation Status: {STATUS}

> {Clear explanation of what this status means}
>
> **PASSED**: All scenarios executed successfully - feature is ready
> **FAILED**: Some scenarios failed - issues must be fixed (see details below)
> **INCOMPLETE**: Validation could not complete - execution errors occurred
> **PARTIAL**: Some scenarios were skipped - cannot fully assess quality

## Summary

| Metric | Value |
|--------|-------|
| User Stories Tested | 3/4 |
| Scenarios Executed | 12 |
| Passed | 10 |
| Failed | 2 |
| Skipped | 1 |
| Execution Errors | 0 |
| **Pass Rate** | **83%** |
| **Overall Status** | **FAILED** |

## Results by User Story

### ‚úÖ US1 - User Authentication (P1)

| Scenario | Status | Duration |
|----------|--------|----------|
| Successful login | ‚úÖ Pass | 2.3s |
| Invalid password | ‚úÖ Pass | 1.8s |
| Account locked | ‚úÖ Pass | 1.5s |

### ‚ö†Ô∏è US2 - Order Management (P2)

| Scenario | Status | Duration |
|----------|--------|----------|
| Create order | ‚úÖ Pass | 3.1s |
| Cancel order | ‚ùå Fail | 2.0s |
| Order history | ‚úÖ Pass | 2.5s |

**Failure Details**:

#### Cancel order - FAILED

- **Step**: When user clicks cancel button
- **Error**: Element `#cancel-btn` not found
- **Screenshot**: `validation/screenshots/us2-cancel-fail.png`
- **Logs**: No errors in backend

### ‚ùå US3 - Reporting (P3)

| Scenario | Status | Duration |
|----------|--------|----------|
| Generate report | ‚ùå Fail | 5.0s |

**Failure Details**:

#### Generate report - FAILED

- **Step**: Then PDF is downloaded
- **Error**: Timeout waiting for download
- **Backend Log**: `[ERROR] ReportService: Template not found`

## Failed Scenarios Summary

| Story | Scenario | Error | Impact |
|-------|----------|-------|--------|
| US2 | Cancel order | Element not found | Medium |
| US3 | Generate report | Template missing | High |

## Recommendations

### Critical (Block Release)
1. Fix report template issue in `ReportService`

### High Priority
1. Add cancel button to order detail page

### Observations
- Login flow is solid
- API response times are good (<500ms)
- Consider adding loading states for better UX
```

### Step 4.2: Save Screenshots

Save all screenshots to `FEATURE_DIR/validation/screenshots/`:

```
validation/
‚îú‚îÄ‚îÄ report-2024-01-15.md
‚îî‚îÄ‚îÄ screenshots/
    ‚îú‚îÄ‚îÄ us1-login-success.png
    ‚îú‚îÄ‚îÄ us2-cancel-fail.png
    ‚îî‚îÄ‚îÄ us3-report-fail.png
```

### Step 4.3: Create Bug Reports

**CRITICAL**: For each failure or issue found, create an individual bug report file in `FEATURE_DIR/validation/bugs/`.

This directory is the SOURCE OF TRUTH for the `/speckit.fix` command. Each bug gets its own file.

#### Bug File Format

Create one file per bug: `validation/bugs/BUG-{number}-{short-desc}.md`

```markdown
---
status: open
severity: critical
user_story: US3
scenario: Generate report
created: {current_date}
---

# BUG-001: Report template not found

## Summary

When generating a PDF report, the system fails with "Template not found" error.

## Reproduction Steps

1. Navigate to Reports section
2. Click "Generate Monthly Report"
3. Select date range
4. Click "Download PDF"

## Expected Behavior

PDF file downloads with the monthly report data.

## Actual Behavior

Error displayed: "Template not found"
Backend log: `[ERROR] ReportService: Template not found at /templates/monthly.ftl`

## Evidence

- Screenshot: `../screenshots/us3-report-fail.png`
- Backend logs attached below

## Technical Analysis

**Probable Cause**: Template file missing or path misconfigured
**Affected Files**: `src/services/ReportService.java:45`
**Suggested Fix**: Check template path configuration in `application.yml`

## Metadata

- **Detected During**: Integration validation
- **Validation Report**: `report-{date}.md`
- **Blocking**: Release
```

#### Bug Severity Levels

| Severity | Criteria | Example |
|----------|----------|---------|
| **critical** | Core functionality broken, no workaround | Login fails, data corruption |
| **high** | Important feature broken | Cancel button missing, export fails |
| **medium** | Feature works with issues | Slow performance, UI glitch |
| **low** | Minor issue, cosmetic | Typo, alignment issue |

#### Bug Status Values

| Status | Meaning |
|--------|---------|
| **open** | Bug identified, not yet addressed |
| **in_progress** | Fix is being worked on |
| **resolved** | Fix applied and verified |
| **wont_fix** | Decided not to fix (documented reason) |

Create bug files for ALL failures found during validation:

```
validation/
‚îú‚îÄ‚îÄ report-{date}.md
‚îú‚îÄ‚îÄ screenshots/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ bugs/
    ‚îú‚îÄ‚îÄ BUG-001-report-template-missing.md
    ‚îú‚îÄ‚îÄ BUG-002-cancel-button-not-found.md
    ‚îî‚îÄ‚îÄ ...
```

### Step 4.4: Create Correction Tasks

If failures found, update `tasks.md` with correction tasks (using smart insertion from `/speckit.review`):

```markdown
### üîß Validation Corrections (Added {date})

> **Source**: Integration validation report
> **Must complete before**: Release

- [ ] T089 [CRITICAL] [US3] Fix missing report template in ReportService
- [ ] T090 [HIGH] [US2] Add cancel button to OrderDetail component
```

---

## Phase 5: Cleanup

### Step 5.1: Stop Services

```
stop_all          ‚Üí Stop all application services
stop_docker       ‚Üí Stop Docker containers
browser_close     ‚Üí Close browser
```

### Step 5.2: Reset Test Data (optional)

```bash
npm run db:reset
```

Or leave data for debugging if failures occurred.

---

## Output

Present to user with CLEAR status indication:

**CRITICAL**: The output MUST clearly indicate whether the feature is ready or not. Do NOT say "Validation Complete" if there are failures or errors.

```markdown
## Validation Result: {STATUS}

{Use appropriate header based on status:}
- ‚úÖ **VALIDATION PASSED** - Feature is ready
- ‚ùå **VALIDATION FAILED** - Issues must be fixed
- ‚ö†Ô∏è **VALIDATION INCOMPLETE** - Could not test all scenarios
- ‚ö†Ô∏è **VALIDATION PARTIAL** - Some scenarios skipped

**Scenarios**: {passed}/{total} passed ({pass_rate}%)

### Status by User Story

‚úÖ **US1 - User Authentication**: All passed
‚ùå **US2 - Order Management**: 1 failure (cancel button missing)
‚ùå **US3 - Reporting**: 1 failure (template error)

### Issues Found

> **IMPORTANT**: These issues MUST be fixed before the feature can be considered complete.

| # | User Story | Scenario | Issue | Severity |
|---|------------|----------|-------|----------|
| 1 | US2 | Cancel order | Element `#cancel-btn` not found | HIGH |
| 2 | US3 | Generate report | Template not found in ReportService | CRITICAL |

### Execution Errors (if any)

> These errors prevented some scenarios from being tested. The feature cannot be validated until these are resolved.

| # | User Story | Error | Impact |
|---|------------|-------|--------|
| - | - | - | - |

### Files Generated

- `validation/report-{date}.md` - Full report with all details
- `validation/screenshots/` - Evidence for failures
- `validation/bugs/` - Individual bug reports (used by `/speckit.fix`)

### Bug Reports Created

> These bug files are the input for `/speckit.fix`. Run fix without arguments to process all open bugs.

| Bug ID | Severity | User Story | Issue |
|--------|----------|------------|-------|
| BUG-001 | CRITICAL | US3 | Report template not found |
| BUG-002 | HIGH | US2 | Cancel button missing |

### Correction Tasks Added

- T089: Fix report template (CRITICAL)
- T090: Add cancel button (HIGH)

### Required Actions

{Based on status:}

**If FAILED or INCOMPLETE**:
> The feature is NOT ready. Run `/speckit.fix` to diagnose and fix the issues.

**If PASSED**:
> The feature is ready for release.

### Next Steps

- [Diagnose & Fix] ‚Üí `/speckit.fix` to analyze and fix issues
- [Re-validate] ‚Üí `/speckit.validate` after fixes
- [Review Report] ‚Üí Open `validation/report-{date}.md`
```

**NEVER present a "Validation Complete" message that could be mistaken for success when there are failures.**

---

## Fallback: No MCP Server

If MCP server is not configured, use bash/tmux:

```bash
# Start services in background
tmux new-session -d -s backend 'cd backend && ./mvnw spring-boot:run'
tmux new-session -d -s frontend 'cd frontend && npm run dev'

# Wait for services
while ! curl -s http://localhost:8080/health > /dev/null; do sleep 1; done

# Run tests with curl for API
curl -X POST http://localhost:8080/api/auth/login -d '{"email":"test@example.com"}'

# For browser tests, recommend installing MCP
echo "For browser automation, run /speckit.mcp first"
```

Recommend running `/speckit.mcp` for full automation capabilities.
